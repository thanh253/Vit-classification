# -*- coding: utf-8 -*-
"""Vision Transformer  for Mango Leaf

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uo566mIRnSL3ISQx4JoZZJZIYpPK7GLV
"""

!pip install roboflow
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt



from roboflow import Roboflow
rf = Roboflow(api_key="Jcx2zwHiPPDoSLGfSiX8")
project = rf.workspace("trn-tin-thnh").project("mango_leaf")
version = project.version(1)
dataset = version.download("folder")





import os
import tensorflow as tf
import keras
from keras import layers
import numpy as np
import matplotlib.pyplot as plt

# Thiết lập các thông số
os.environ["KERAS_BACKEND"] = "jax"  # hoặc "tensorflow", "torch",'jax'

import os

train_dir = '/content/mango_leaf--1/train'
val_dir = '/content/mango_leaf--1/valid'
test_dir = '/content/mango_leaf--1/test'

# Tạo một từ điển để ánh xạ từ tên thư mục sang các số nguyên duy nhất
label_to_index = {}
index = 0
for label in os.listdir(train_dir):
    label_to_index[label] = index
    index += 1
num_labels = len(label_to_index)
print("Number of unique labels:", num_labels)
# Đọc dữ liệu từ các tệp trong các thư mục train, val, và test
def read_data_from_directory(directory, label_to_index):
    data = []
    labels = []
    for label in os.listdir(directory):
        label_index = label_to_index[label]
        label_dir = os.path.join(directory, label)
        for file_name in os.listdir(label_dir):
            file_path = os.path.join(label_dir, file_name)
            # Read and preprocess the image
            img = tf.keras.preprocessing.image.load_img(file_path, target_size=(72, 72))
            img_array = tf.keras.preprocessing.image.img_to_array(img)
            img_array = img_array / 255.0  # Normalize pixel values to [0, 1]
            data.append(img_array)
            labels.append(label_index)
    return np.array(data), np.array(labels)


# Đọc dữ liệu từ các thư mục train, val, và test
x_train, y_train = read_data_from_directory(train_dir, label_to_index)
x_val, y_val = read_data_from_directory(val_dir, label_to_index)
x_test, y_test = read_data_from_directory(test_dir, label_to_index)

# In ra các thông tin về kích thước của dữ liệu mới
print(f"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}")
print(f"x_val shape: {x_val.shape} - y_val shape: {y_val.shape}")
print(f"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}")

label_to_index = {}
index = 0
for label in os.listdir(train_dir):
    label_to_index[label] = index
    index += 1

# In số lượng label
num_labels = len(label_to_index)
print("Number of unique labels:", num_labels)

weight_decay = 0.0001
learning_rate = 0.001
batch_size = 72
num_epochs = 30  # For real training, use num_epochs=100. 10 is a test value
image_size = 40  # We'll resize input images to this size
patch_size = 4  # Size of the patches to be extract from the input images
num_patches = (image_size // patch_size) ** 2
projection_dim = 64
num_heads = 4
transformer_units = [
    projection_dim * 2,
    projection_dim,
]  # Size of the transformer layers
transformer_layers = 8
mlp_head_units = [
    2048,
    1024,
]  # Size of the dense layers of the final classifier

data_augmentation = tf.keras.Sequential(
    [
        layers.experimental.preprocessing.Normalization(),
        layers.experimental.preprocessing.Resizing(image_size, image_size),
        layers.experimental.preprocessing.RandomFlip("horizontal"),
        layers.experimental.preprocessing.RandomRotation(factor=0.02),
        layers.experimental.preprocessing.RandomZoom(height_factor=0.2, width_factor=0.2),
    ],
    name="data_augmentation",
)

data_augmentation.layers[0].adapt(x_train)

def mlp(x, hidden_units, dropout_rate):
    for units in hidden_units:
        x = layers.Dense(units, activation=keras.activations.gelu)(x)
        x = layers.Dropout(dropout_rate)(x)  # Add Dropout regularization
    return x

# Patch creation layer
class Patches(layers.Layer):
    def __init__(self, patch_size):
        super().__init__()
        self.patch_size = patch_size

    def call(self, images):
        input_shape = tf.shape(images)
        batch_size = input_shape[0]
        height = input_shape[1]
        width = input_shape[2]
        channels = input_shape[3]
        num_patches_h = height // self.patch_size
        num_patches_w = width // self.patch_size
        patches = tf.image.extract_patches(images, sizes=[1, self.patch_size, self.patch_size, 1], strides=[1, self.patch_size, self.patch_size, 1], rates=[1, 1, 1, 1], padding='VALID')
        patches = tf.reshape(
            patches,
            (
                batch_size,
                num_patches_h * num_patches_w,
                self.patch_size * self.patch_size * channels,
            ),
        )
        return patches

    def get_config(self):
        config = super().get_config()
        config.update({"patch_size": self.patch_size})
        return config

# Patch encoding layer
class PatchEncoder(layers.Layer):
    def __init__(self, num_patches, projection_dim):
        super().__init__()
        self.num_patches = num_patches
        self.projection = layers.Dense(units=projection_dim)
        self.position_embedding = layers.Embedding(
            input_dim=num_patches, output_dim=projection_dim
        )

    def call(self, patch):
        positions = tf.expand_dims(
            tf.range(start=0, limit=self.num_patches, delta=1), axis=0
        )
        projected_patches = self.projection(patch)
        encoded = projected_patches + self.position_embedding(positions)
        return encoded

    def get_config(self):
        config = super().get_config()
        config.update({"num_patches": self.num_patches})
        return config

# Updated ViT model with Layer Normalization and Learning Rate Scheduler
def create_vit_classifier():
    input_shape = (72, 72, 3)
    num_classes = len(np.unique(y_train))
    inputs = keras.Input(shape=input_shape)
    augmented = data_augmentation(inputs)
    patches = Patches(patch_size)(augmented)
    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)

    for _ in range(transformer_layers):
        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)  # Add Layer Normalization
        attention_output = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=projection_dim, dropout=0.1
        )(x1, x1)
        x2 = layers.Add()([attention_output, encoded_patches])
        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)  # Add Layer Normalization
        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)
        encoded_patches = layers.Add()([x3, x2])

    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)
    representation = layers.Flatten()(representation)
    representation = layers.Dropout(0.5)(representation)
    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)
    logits = layers.Dense(num_classes)(features)
    model = keras.Model(inputs=inputs, outputs=logits)

    # Learning Rate Scheduler
    initial_learning_rate = 0.001
    lr_schedule = keras.optimizers.schedules.ExponentialDecay(
        initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True
    )
    optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)
    model.compile(
        optimizer=optimizer,
        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=[
            keras.metrics.SparseCategoricalAccuracy(name="accuracy"),
            keras.metrics.SparseTopKCategoricalAccuracy(5, name="top-5-accuracy"),
        ],
    )

    return model

from keras.callbacks import EarlyStopping
# Compile, train, and evaluate the model
def run_experiment(model):
    optimizer = keras.optimizers.AdamW(
         learning_rate=learning_rate, weight_decay=weight_decay
    )

    model.compile(
        optimizer=optimizer,
        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=[
            keras.metrics.SparseCategoricalAccuracy(name="accuracy"),
        ],
    )

    checkpoint_filepath = "/tmp/checkpoint.weights"
    checkpoint_callback = keras.callbacks.ModelCheckpoint(
        checkpoint_filepath,
        monitor="val_accuracy",
        save_best_only=True,
        save_weights_only=True,
    )

    early_stopping = EarlyStopping(monitor='val_loss',
                               patience=5,
                               restore_best_weights=True,
                               mode='min',
    )

    history = model.fit(
        x=x_train,
        y=y_train,
        batch_size=batch_size,
        epochs=num_epochs,
        validation_data=(x_val, y_val),
        callbacks=[checkpoint_callback, early_stopping],
    )

    model.load_weights(checkpoint_filepath)
    train_loss, train_accuracy = model.evaluate(x_train, y_train, verbose=0)
    val_loss, val_accuracy = model.evaluate(x_val, y_val, verbose=0)
    print("Training Loss: {:.4f}, Accuracy: {:.4f}".format(train_loss, train_accuracy))
    print("Validation Loss: {:.4f}, Accuracy: {:.4f}".format(val_loss, val_accuracy))

    test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)
    print("Test Loss:", test_loss)
    print("Test Accuracy:", test_accuracy)
    return history
vit_classifier = create_vit_classifier()
history = run_experiment(vit_classifier)

# Define needed variables
tr_acc = history.history['accuracy']
tr_loss = history.history['loss']
val_acc = history.history['val_accuracy']
val_loss = history.history['val_loss']
index_loss = np.argmin(val_loss)
val_lowest = val_loss[index_loss]
index_acc = np.argmax(val_acc)
acc_highest = val_acc[index_acc]
epochs = range(1, len(tr_acc) + 1)
loss_label = f'best epoch= {str(index_loss + 1)}'
acc_label = f'best epoch= {str(index_acc + 1)}'

# Plot training history
plt.figure(figsize=(20, 8))
plt.style.use('fivethirtyeight')

plt.subplot(1, 2, 1)
plt.plot(epochs, tr_loss, 'r', label='Training loss')
plt.plot(epochs, val_loss, 'g', label='Validation loss')
plt.scatter(index_loss + 1, val_lowest, s=150, c='blue', label=loss_label)
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(epochs, tr_acc, 'r', label='Training Accuracy')
plt.plot(epochs, val_acc, 'g', label='Validation Accuracy')
plt.scatter(index_acc + 1, acc_highest, s=150, c='blue', label=acc_label)
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

from sklearn.metrics import confusion_matrix, classification_report
import itertools

# Calculate the length of the test dataset
ts_length = len(x_test)

# Determine the appropriate batch size for testing
test_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1) if ts_length%n == 0 and ts_length/n <= 80]))

# Calculate the number of steps per epoch for testing
test_steps = ts_length // test_batch_size

# Evaluate the model on the training, validation, and test sets
train_score = vit_classifier.evaluate(x_train, y_train, batch_size=test_batch_size, verbose=1)
valid_score = vit_classifier.evaluate(x_val, y_val, batch_size=test_batch_size, verbose=1)
test_score = vit_classifier.evaluate(x_test, y_test, batch_size=test_batch_size, verbose=1)

# Print the evaluation results
print("Train Loss: ", train_score[0])
print("Train Accuracy: ", train_score[1])
print('-' * 20)
print("Validation Loss: ", valid_score[0])
print("Validation Accuracy: ", valid_score[1])
print('-' * 20)
print("Test Loss: ", test_score[0])
print("Test Accuracy: ", test_score[1])

# Make predictions on test images
preds = vit_classifier.predict(x_test, batch_size=test_batch_size)
y_pred = np.argmax(preds, axis=1)

# Get the class labels
labels = {v: k for k, v in label_to_index.items()}
classes = list(labels.values())

# Generate confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Plot confusion matrix
plt.figure(figsize=(10, 10))
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.colorbar()
tick_marks = np.arange(len(classes))
plt.xticks(tick_marks, classes, rotation=45)
plt.yticks(tick_marks, classes)
thresh = cm.max() / 2.
for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
    plt.text(j, i, cm[i, j], horizontalalignment='center', color='white' if cm[i, j] > thresh else 'black')
plt.tight_layout()
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.show()

from sklearn.metrics import classification_report

# Assuming y_test contains true labels and y_pred contains predicted labels
# Also assuming you have defined `classes` as a list of class names

print(classification_report(y_test, y_pred, labels=range(len(classes)), target_names=classes))

import numpy as np
import matplotlib.pyplot as plt
from PIL import Image  # Import PIL for handling images

# Make predictions on test images
pred = vit_classifier.predict(x_test)

# Get the predicted labels
pred_labels = np.argmax(pred, axis=1)

# Map numeric labels to class names
labels = {v: k for k, v in label_to_index.items()}
pred_class_names = [labels[i] for i in pred_labels]

print(f'The first 5 predictions: {pred_class_names[:5]}')

# Display 15 random pictures from the dataset with their labels
random_index = np.random.randint(0, len(x_test) - 1, 15)
fig, axes = plt.subplots(nrows=3, ncols=5, figsize=(25, 15),
                         subplot_kw={'xticks': [], 'yticks': []})

for i, ax in enumerate(axes.flat):
    image = Image.fromarray((x_test[random_index[i]] * 255).astype(np.uint8))  # Convert normalized image back to uint8
    ax.imshow(image)
    true_label = labels[y_test[random_index[i]]]
    pred_label = pred_class_names[random_index[i]]
    color = "green" if true_label == pred_label else "red"
    ax.set_title(f"True: {true_label}\nPredicted: {pred_label}", color=color)

plt.tight_layout()  # Adjust layout to prevent overlap
plt.show()

# Separate correct and incorrect predictions
correct_indices = []
incorrect_indices = []
for i in range(len(y_test)):
    true_label = labels[y_test[i]]
    pred_label = pred_class_names[i]
    if true_label == pred_label:
        correct_indices.append(i)
    else:
        incorrect_indices.append(i)

# Display 15 correct predictions
fig, axes = plt.subplots(nrows=3, ncols=5, figsize=(25, 15),
                         subplot_kw={'xticks': [], 'yticks': []})

for i, ax in enumerate(axes.flat):
    if i < len(correct_indices):
        idx = correct_indices[i]
        image = Image.fromarray((x_test[idx] * 255).astype(np.uint8))
        ax.imshow(image)
        true_label = labels[y_test[idx]]
        pred_label = pred_class_names[idx]
        ax.set_title(f"True: {true_label}\nPredicted: {pred_label}", color="green")

plt.tight_layout()
plt.show()

# Display 15 incorrect predictions
fig, axes = plt.subplots(nrows=3, ncols=5, figsize=(25, 15),
                         subplot_kw={'xticks': [], 'yticks': []})

for i, ax in enumerate(axes.flat):
    if i < len(incorrect_indices):
        idx = incorrect_indices[i]
        image = Image.fromarray((x_test[idx] * 255).astype(np.uint8))
        ax.imshow(image)
        true_label = labels[y_test[idx]]
        pred_label = pred_class_names[idx]
        ax.set_title(f"True: {true_label}\nPredicted: {pred_label}", color="red")

plt.tight_layout()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from PIL import Image  # Import PIL for handling images

# Make predictions on test images
pred = vit_classifier.predict(x_test)

# Get the predicted labels
pred_labels = np.argmax(pred, axis=1)

# Map numeric labels to class names
labels = {v: k for k, v in label_to_index.items()}
pred_class_names = [labels[i] for i in pred_labels]

print(f'The first 5 predictions: {pred_class_names[:5]}')

# Find indices of correct and incorrect predictions
correct_indices = np.where(pred_labels == y_test)[0]
incorrect_indices = np.where(pred_labels != y_test)[0]

# Randomly select 15 correct and 15 incorrect indices
random_correct_indices = np.random.choice(correct_indices, 15, replace=False)
random_incorrect_indices = np.random.choice(incorrect_indices, 15, replace=False)

# Concatenate the selected indices
selected_indices = np.concatenate([random_correct_indices, random_incorrect_indices])

# Display selected images with their labels
fig, axes = plt.subplots(nrows=6, ncols=5, figsize=(25, 20),
                         subplot_kw={'xticks': [], 'yticks': []})

for i, ax in enumerate(axes.flat):
    image = Image.fromarray((x_test[selected_indices[i]] * 255).astype(np.uint8))  # Convert normalized image back to uint8
    ax.imshow(image)
    true_label = labels[y_test[selected_indices[i]]]
    pred_label = pred_class_names[selected_indices[i]]
    color = "green" if true_label == pred_label else "red"
    ax.set_title(f"True: {true_label}\nPredicted: {pred_label}", color=color)

plt.tight_layout()  # Adjust layout to prevent overlap
plt.show()